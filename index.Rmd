---
title: "La percepción de corrupción a nivel global - Años 2019 y 2020"
subtitle: 'Curso: POL304 - Estadística para el análisis político 2'
author: "Santiago Maldonado Vizcarra, Andrea Salas y Ana Goñi"
date: "Noviembre, 2022"
output:
 prettydoc::html_pretty:
  theme: leonids
  highlight: github
---
 

```{r include=FALSE}
library(rio)
library(dplyr)
library(ggplot2)
library(cowplot)
library(kableExtra)
library(equatiomatic)
library(tables)
library(rmarkdown)
library(reshape)
library(DT)
library(modelsummary)
library(factoextra)
library(cluster)
library(tidyverse)
library(hrbrthemes)
```


El objetivo del presente trabajo es analizar los resultados de la percepción de corrupción a nivel global en los años 2019 y 2020. Escogimos este periodo temporal por el cambio drástico que generó la propagación del virus COVID-19 en el desarrollo económico, político y social. En ese sentido, intentamos determinar si la pandemia realmente tuvo un impacto significativo en la percepción ciudadana.  


# 1. Datos previos al análisis:


+ La variable dependiente escogida para este estudio es la "Percepción de corrupción" (PERCOR). 

+ Se ha eliminado la variable independiente "Saneamiento", esto debido a que la base de datos ofrecía muy pocos casos para el análisis. 

+ Las variables que se mantienen para el análisis son las siguientes:

  --> Independencia de justicia (INDEP)
  --> Gobierno electo (ELECT)
  --> Ausencia de corrupción (AUSCORR)
  --> Desempleo (DES)
  --> Inflación (INFLA)
  --> Indice de democracia (PAR)
  --> Indice de Desarrollo Humano (IDH)
  --> Legitimidad estatal (LEG)

+ Se realizará un análisis descriptivo y georreferenciado de los resultados a nivel de países

+ Se aplicará técnicas de reducción de dimensiones para agrupar a los países en función de las variables presentadas anteriormente. Estas reflejan aspectos sociales,   económicos y políticos de los diferentes estados presentes en la base de datos. 

+ Se implementarán técnicas multivariadas para modelar el comportamiento de la "Percepción de la corrupción" en los distintos países.

# 2. Base de datos unificada:


```{r include=FALSE}
LinkBDU = "https://github.com/santiagomv29/PRACTICAS_ESTA2/raw/main/dataunificada.csv"
library(rio)
BDU = import(LinkBDU)
```

```{r echo=FALSE}
datatable(BDU)
```


## 2.1. Presentación de la variable dependiente - Año 2019:


```{r echo=FALSE}
g2019 = barplot(BDU$`2019PERCOR`,main="Percepción de corrupción - Año 2019",ylab="Puntaje",xlab= NULL,las=2, ylim= c(0,100),width = 1, space = 1, names.arg = BDU$COUNTRY, col = "skyblue", border = "black") 
```


## 2.2. Presentación de la variable dependiente - Año 2020: 


```{r echo=FALSE}
g2020 = barplot(BDU$`2020PERCOR`,main="Percepción de corrupción - Año 2019",ylab="Puntaje",xlab= NULL,las=2, ylim= c(0,100), width = 1, space = 1, names.arg = BDU$COUNTRY, col = "lightgreen", border = "black")
```


# 3. Mapas de calor con variable dependiente: 


```{r include=FALSE} 
library(sp) 
library(rgdal)
library(ggplot2)
mapDIS=sf::read_sf("C:/Users/Santiago/OneDrive/Documentos/GitHub/PRACTICAS_ESTA2/fwdshapefile/world-administrative-boundaries.shp") 

```

```{r include=FALSE}
MAP = merge(mapDIS,BDU, by.x = "iso3", by.y = "ISO", all.x = T)
```


## 3.1. Mapa de calor - Año 2019:

```{r echo=FALSE}

mapaleyendaL= ggplot(MAP)+ geom_sf() + theme_light()

mapaleyL= mapaleyendaL + geom_sf(data=MAP,
              aes(fill=`2019PERCOR`),color = NA)
      
mapa3= mapaleyL +
coord_sf() + 
scale_fill_gradient(low = "seashell",  high = "firebrick", breaks=seq(from=0, to=1, by=.2)) + theme_void() + 
  
theme(axis.title = element_blank(), axis.text = element_blank(), legend.position = c(1.1,0.55)) + labs(fill=" ") + theme(legend.text = element_text(size = 13)) +
  
labs(title = "Percepción de corrupción - 2019", subtitle = "") +
  
theme(
plot.title = element_text(color="black", size=15, face="bold"),
plot.caption = element_text(color = "black", size=10))

mapa3 
```


## 3.2. Mapa de calor - Año 2020:


```{r echo=FALSE}

mapaleyendaL2= ggplot(MAP)+ geom_sf() + theme_light()

mapaleyL2= mapaleyendaL2 + geom_sf(data=MAP,
              aes(fill=`2020PERCOR`),color = NA)
      
mapa4= mapaleyL2 +
coord_sf() + 
scale_fill_gradient(low = "seashell",  high = "firebrick", breaks=seq(from=0, to=1, by=.2)) + theme_void() + 
  
theme(axis.title = element_blank(), axis.text = element_blank(), legend.position = c(1.1,0.55)) + labs(fill=" ") + theme(legend.text = element_text(size = 13)) +
  
labs(title = "Percepción de corrupción - 2020", subtitle = "") +
  
theme(
plot.title = element_text(color="black", size=15, face="bold"),
plot.caption = element_text(color = "black", size=10))

mapa4
```


# 4. Conglomeración de variables independientes: 


## 4.1. Preparación de datos para la conglomeración - Año 2019: 


```{r echo=FALSE}
BDU2=BDU[,-c(4,6,8,10,12,13,14,16,18,20)]
datatable(BDU2)
```

```{r include=FALSE}
boxplot(BDU2[,c(3:10)],horizontal = F,las=2,cex.axis =0.5)
```

```{r include=FALSE}
library(BBmisc)
boxplot(normalize(BDU2[,c(3:10)],method='range',range=c(0,10))) #CAMBIAR ORIENTACIÓN DE NOMBRES
```

```{r include=FALSE}
library(BBmisc)
boxplot(normalize(BDU2[,c(3:10)],method='standardize'))
```

```{r include=FALSE}
BDU2[,c(3:10)]=normalize(BDU2[,c(3:10)],method='standardize') #preguntar si nos quedamos con este o cambiamos por el de range
```

```{r include=FALSE}
cor(BDU2[,c(3:10)])
```


```{r include=FALSE}
clusBDU2=BDU2[,c(3:10)] #AQUÍ INICIA LA TÉCNICA PAM
row.names(clusBDU2)=BDU2$COUNTRY
```

```{r include=FALSE}
library(cluster)
g.dist = daisy(clusBDU2, metric="gower")
```

```{r include=FALSE}
library(factoextra)
fviz_nbclust(clusBDU2, pam,diss=g.dist,method ="gap_stat",k.max =10,verbose =F)
```

```{r include=FALSE}
library(dplyr)
library(kableExtra) #rango entre 3 y 5 (elegir 5 por que es el más cercano a 9) /VOLVER FACTOR EL VECTOR DE CLUSTER
set.seed(123)
res.pam=pam(g.dist,5,cluster.only=F) #el número refleja la cantidad de clusters
#nueva columna
clusBDU2$pam=res.pam$cluster
# ver
head(clusBDU2,15)%>%kbl()%>%kable_styling()

```

```{r include=FALSE}
fviz_silhouette(res.pam,print.summary = F) #barras para abajo están mal clusterizadas
```


## 4.2. Conglomeración con técnica aglomerativa AGNES - Año 2019:


```{r echo=FALSE}
fviz_nbclust(clusBDU2, hcut,diss=g.dist,method = "gap_stat",k.max = 10,verbose = F,hc_func = "agnes")
```

```{r echo=FALSE}
set.seed(123)
library(factoextra)
res.agnes<- hcut(g.dist, k =5,hc_func='agnes',hc_method ="ward.D")
clusBDU2$agnes=res.agnes$cluster
# ver
head(clusBDU2,15)%>%kbl()%>%kable_styling()
```

```{r include=FALSE}
fviz_dend(res.agnes, cex = 0.7, horiz = T,main = "")
```

```{r echo=FALSE}
fviz_silhouette(res.agnes,print.summary = F)
```


```{r include=FALSE}
fviz_nbclust(clusBDU2, hcut,diss=g.dist,method ="gap_stat",k.max = 10,verbose = F,hc_func ="diana") #AQUÍ INICIA DIANA
```

```{r include=FALSE}
set.seed(123)
res.diana <- hcut(g.dist, k = 3,hc_func='diana') #se elige 3 por ser el número más cercano a 1 
clusBDU2$diana=res.diana$cluster
# veamos
head(clusBDU2,15)%>%kbl%>%kable_styling()
```

```{r include=FALSE}
fviz_dend(res.diana, cex =0.7, horiz = T, main ="")
```

```{r include=FALSE}
fviz_silhouette(res.diana,print.summary = F)
```


## 4.3. Preparación de datos para la conglomeración - Año 2020: 


```{r echo=FALSE}
BDU3=BDU[,-c(3,5,7,9,11,13,14,15,17,19)]
datatable(BDU3) 
```

```{r include=FALSE}
boxplot(BDU3[,c(3:10)],horizontal = F,las=2,cex.axis =0.5)
```

```{r include=FALSE}
boxplot(normalize(BDU3[,c(3:10)],method='range',range=c(0,10))) #CAMBIAR ORIENTACIÓN DE NOMBRES
```

```{r include=FALSE}
library(BBmisc)
boxplot(normalize(BDU3[,c(3:10)],method='standardize'))
```

```{r include=FALSE}
BDU3[,c(3:10)]=normalize(BDU3[,c(3:10)],method='standardize') #preguntar si nos quedamos con este o cambiamos por el de range
```

```{r include=FALSE}
cor(BDU3[,c(3:10)])
```


```{r include=FALSE}
clusBDU3=BDU3[,c(3:10)] #AQUÍ INICIA PAM
row.names(clusBDU3)=BDU3$COUNTRY
```

```{r include=FALSE}
library(cluster)
g.dist2 = daisy(clusBDU3, metric="gower")
```

```{r include=FALSE}
library(factoextra)
fviz_nbclust(clusBDU3, pam,diss=g.dist2,method ="gap_stat",k.max =10,verbose =F)
```

```{r include=FALSE}
library(dplyr)
library(kableExtra)
set.seed(123)
res.pam2=pam(g.dist2,5,cluster.only=F) #el número refleja la cantidad de clusters (se elige 5 por que es el más cercano a 9)
#nueva columna
clusBDU3$pam=res.pam2$cluster
# ver
head(clusBDU3,15)%>%kbl()%>%kable_styling()

```


## 4.4. Conglomeración con técnica aglomerativa AGNES - Año 2020:


```{r echo=FALSE}
fviz_nbclust(clusBDU3, hcut,diss=g.dist2,method = "gap_stat",k.max = 10,verbose = F,hc_func = "agnes")
```

```{r echo=FALSE}
set.seed(123)
library(factoextra)
res.agnes2<- hcut(g.dist2, k =3,hc_func='agnes',hc_method ="ward.D")
clusBDU3$agnes=res.agnes2$cluster
# ver
head(clusBDU3,15)%>%kbl()%>%kable_styling()
```

```{r include=FALSE}
fviz_dend(res.agnes2, cex = 0.7, horiz = T,main = "")
```

```{r echo=FALSE}
fviz_silhouette(res.agnes2,print.summary = F)
```


```{r include=FALSE}
fviz_nbclust(clusBDU3, hcut,diss=g.dist2,method ="gap_stat",k.max = 10,verbose = F,hc_func ="diana") #AQUÍ INICIA DIANA
```

```{r include=FALSE}
set.seed(123)
res.diana2 <- hcut(g.dist2, k = 3,hc_func='diana')
clusBDU3$diana=res.diana2$cluster
# veamos
head(clusBDU3,15)%>%kbl%>%kable_styling()
```

```{r include=FALSE}
fviz_dend(res.diana2, cex =0.7, horiz = T, main ="")
```

```{r include=FALSE}
fviz_silhouette(res.diana2,print.summary = F)
```


## 4.5. Interpretación de los conglomerados - Años 2019 y 2020:


Vamos a agrupar a los países en función a sus características sociales, económicas y políticas, siguiendo técnicas de reducción de dimensiones. Tenemos tres estrategias: divisiva con la técnica PAM y jerárquica con las técnicas de Agnes y Diana.

Decidimos optar por la estrategia de aglomeración jerárquica AGNES, pues nos permite estandarizar los datos obteniendo el coeficiente de aglomeración, el cual mide la cantidad de estructura de agrupamiento encontrada (los valores más cercanos a 1 sugieren una estructura de agrupación fuerte). Además, ha mostrado ser la técnica con mejor desempeño para el agrupamiento de estos casos. Esto debido a que, si bien los resultados generales obtenidos fueron bajos, esta técnica mostró mejores resultados, siendo de 0.29 para el 2019 y 0.36 para el 2020 respectivamente en las siluetas.

En la técnica AGNES para el año 2019, el número de clusters recomendados para la partición fue de 8, pero se determinamos que 5 sería más adecuado. Siguiendo esta técnica, para el año 2020, solicitamos el número óptimo de clusters y fueron 3 (valor con el cual nos quedamos).

 
# 6. Exploración de los conglomerados con técnica AGNES:


## 6.1. Características de los conglomerados:


## 6.1.1. Año 2019: 


```{r include=FALSE}
BDU2$clust = factor(res.agnes$cluster)
head(BDU2)
```

```{r include=FALSE}
#Lo pasamos a la data original:
BDU2 = BDU2[,c(2,11)]
```

```{r include=FALSE}
BDU4 = BDU[,-c(13,14)]
BDU4
```


```{r include=FALSE}
BDU4 = merge(BDU4, BDU2, by = "COUNTRY")
BDU4
```

```{r include=FALSE}
BDU4$clust = factor(BDU4$clust, levels = c(1:5), labels = c("Corrupto","Medianamente corrupto","Muy poco corrupto","Muy corrupto","Poco corrupto"))
```


```{r include=FALSE}
summ_clust = BDU4 %>% 
  group_by(clust) %>%
  summarise(`2019INDEP` = mean(`2019INDEP`, na.rm=T),
            `2019ELECT` = mean(`2019ELECT`, na.rm = T),
            `2019AUSCORR` = mean(`2019AUSCORR`, na.rm = T),
            `2019PAR` = mean(`2019PAR`, na.rm =T),
            `2019IDH` = mean(`2019IDH`, na.rm = T),
            `2019LEG` = mean(`2019LEG`, na.rm = T),
            `2019DES` = mean(`2019DES`, na.rm = T),
            `2019INFLACION` = mean(`2019INFLACION`, na.rm = T))
```

```{r echo=FALSE}
summ_clust%>%
  kbl() %>%
  kable_minimal()
```


## 6.1.2. Año 2020:


```{r include=FALSE}
BDU3$clust = factor(res.agnes2$cluster)
head(BDU3)
```

```{r include=FALSE}
#Lo pasamos a la data original:
BDU3 = BDU3[,c(2,11)]
```

```{r include=FALSE}
BDU5 = BDU[-c(13,14)]
```

```{r include=FALSE}
BDU5 = merge(BDU5, BDU3, by = "COUNTRY")
BDU5
```

```{r include=FALSE}
BDU5$clust = factor(BDU5$clust, levels = c(1:3), labels = c("Corrupto o Medianamente corrupto","Muy Corrupto", "Poco corrupto"))
```

```{r include=FALSE}
summ_clust = BDU5 %>% 
  group_by(clust) %>%
  summarise(`2020INDEP` = mean(`2020INDEP`, na.rm=T),
            `2020ELECT` = mean(`2020ELECT`, na.rm = T),
            `2020AUSCORR` = mean(`2020AUSCORR`, na.rm = T),
            `2020PAR` = mean(`2020PAR`, na.rm =T),
            `2020IDH` = mean(`2020IDH`, na.rm = T),
            `2020LEG` = mean(`2020LEG`, na.rm = T),
            `2020DES` = mean(`2020DES`, na.rm = T),
            `2020INFLACION` = mean(`2020INFLACION`, na.rm = T))
```

```{r echo=FALSE}
summ_clust%>%
  kbl() %>%
  kable_minimal()
```


## 6.2. Número de países por conglomerado:


## 6.2.1.  Año 2019:


```{r include=FALSE}
bar1 = BDU4 %>%
  group_by(clust) %>%
  summarise(Cluster = n())
```

```{r echo=FALSE,message=FALSE,warning=FALSE,eval=TRUE,fig.show='hold',fig.width=7,fig.height=5, fig.align="center"}
ggplot(bar1, aes(x=clust, y=Cluster)) + 
  geom_bar(stat = "identity") +
  
  labs(title = " ",
       x = " ",
       y = " ") +
    geom_text(aes(label=Cluster), size=4, vjust=1.5, hjust = 0.5, color="white") + 
  
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank())
```


## 6.2.2.  Año 2020:


```{r include=FALSE}
bar2 = BDU5 %>%
  group_by(clust) %>%
  summarise(Cluster = n())
```

```{r echo=FALSE,message=FALSE,warning=FALSE,eval=TRUE,fig.show='hold',fig.width=7,fig.height=5, fig.align="center"}
ggplot(bar2, aes(x=clust, y=Cluster)) + 
  geom_bar(stat = "identity") +
  
  labs(title = " ",
       x = " ",
       y = " ") +
    geom_text(aes(label=Cluster), size=4, vjust=1.5, hjust = 0.5, color="white") + 
  
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank())
```


## 6.3. Mapas de calor: 


## 6.3.1. Año 2019:


```{r include=FALSE}
#Juntamos información con el shape:
mapCOUN19=merge(mapDIS,BDU4,by.x='iso3',by.y='ISO', all.x = T) #siempre primero el shape
```

```{r echo=FALSE,message=FALSE,warning=FALSE,eval=TRUE,fig.show='hold',fig.width=9.5,fig.height=5.55, fig.align="center"}
mapaleyendaL2019= ggplot(mapCOUN19)+ geom_sf() + theme_light()
mapaleyL2019= mapaleyendaL2019 + geom_sf(BDU4=mapCOUN19,
              aes(fill=clust),color = "gray")
      
mapa1= mapaleyL2019 +
coord_sf() + 
scale_fill_manual(values=c("tomato","turquoise","lightgoldenrod1", "lightgreen","yellow")) + theme_void() +
  
  
theme(axis.title = element_blank(), axis.text = element_blank(), legend.position = "right") + labs(fill=" ") + theme(legend.text = element_text(size = 8)) +
  
labs(title = "Países según conglomerado de Percepción de corrupción") +
  
theme(
plot.title = element_text(color="black", size=11, face="bold"))
mapa1
```


## 6.3.2. Año 2020: 


```{r include=FALSE}
#Juntamos información con el shape:
mapCOUN20=merge(mapDIS,BDU5,by.x='iso3',by.y='ISO', all.x = T) #siempre primero el shape
```

```{r echo=FALSE,message=FALSE,warning=FALSE,eval=TRUE,fig.show='hold',fig.width=9.5,fig.height=5.55, fig.align="center"}
mapaleyendaL2020= ggplot(mapCOUN20)+ geom_sf() + theme_light()
mapaleyL2020= mapaleyendaL2020 + geom_sf(BDU5=mapCOUN20,
              aes(fill=clust),color = "gray")
      
mapa2= mapaleyL2020 +
coord_sf() + 
scale_fill_manual(values=c("tomato","turquoise","lightgoldenrod1")) + theme_void() +
  
  
theme(axis.title = element_blank(), axis.text = element_blank(), legend.position = "right") + labs(fill=" ") + theme(legend.text = element_text(size = 8)) +
  
labs(title = "Países según conglomerado de Percepción de corrupción") +
  
theme(
plot.title = element_text(color="black", size=11, face="bold"))
mapa2
```


# 7. Regresiones lineales y poisson:


```{r include=FALSE}
REGRELMTOTAL19 = lm(BDU$`2019PERCOR` ~ BDU$`2019INDEP` + BDU$`2019ELECT` + BDU$`2019AUSCORR` + BDU$`2019DES` + BDU$`2019INFLACION` + BDU$`2019PAR` + BDU$`2019IDH` + BDU$`2019LEG`)
```

```{r include=FALSE}
REGRELMTOTAL20 = lm(BDU$`2020PERCOR` ~ BDU$`2020INDEP` + BDU$`2020ELECT` + BDU$`2020AUSCORR` + BDU$`2020DES` + BDU$`2020INFLACION` + BDU$`2020PAR` + BDU$`2020IDH` + BDU$`2020LEG`)
```

```{r include=FALSE}
REGREPOITOTAL19 = glm(BDU$`2019PERCOR` ~ BDU$`2019INDEP` + BDU$`2019ELECT` + BDU$`2019AUSCORR` + BDU$`2019DES` + BDU$`2019INFLACION` + BDU$`2019PAR` + BDU$`2019IDH` + BDU$`2019LEG`,family = poisson(link = "log"))
```

```{r include=FALSE}
REGREPOITOTAL20 = glm(BDU$`2020PERCOR` ~ BDU$`2020INDEP` + BDU$`2020ELECT` + BDU$`2020AUSCORR` + BDU$`2020DES` + BDU$`2020INFLACION` + BDU$`2020PAR` + BDU$`2020IDH` + BDU$`2020LEG`,family = poisson(link = "log"))
```


## 7.1. Comparación de regresiones lineal y poisson - Año 2019:


```{r echo=FALSE}
modelslmpoi=list('Coeficientes Lineal' =REGRELMTOTAL19, 
                 'Coeficientes asegurada' = REGREPOITOTAL19)
modelsummary(modelslmpoi, title = "Regresiones Lineal y Poisson 2019",
stars = TRUE,
output = "kableExtra")
```


## 7.1.1. Coeficientes exponenciados de regresión poisson - Año 2019:


```{r include=FALSE}
modelspoi19=list('Coeficientes Poisson' =REGREPOITOTAL19)
modelsummary(modelspoi19, title = "Regresión Poisson - Año 2019",
stars = TRUE,
output = "kableExtra")
```

```{r echo=FALSE}
f2019 <- function (x) format(x, digits =4, scientific = FALSE)
modelsummary(modelspoi19,
             fmt=f2019,
             exponentiate = T,
             statistic = 'conf.int',
             title = "Exponenciales de la Regresión Poisson - Año 2019",
             stars =TRUE, 
             output = "kableExtra")
```


## 7.2. Comparación de regresiones lineal y poisson - Año 2020: 


```{r echo=FALSE}
modelslmpoi2=list('Coeficientes Lineal' =REGRELMTOTAL20, 
                 'Coeficientes Poisson' =REGREPOITOTAL20)
modelsummary(modelslmpoi2, title = "Regresiones Lineal y Poisson 2020",
stars = TRUE,
output = "kableExtra")
```


## 7.2.1. Coeficientes exponenciados de regresión poisson - Año 2020: 


```{r include=FALSE}
modelspoi20=list('Coeficientes Poisson' =REGREPOITOTAL20)
modelsummary(modelspoi20, title = "Regresión Poisson - Año 2020",
stars = TRUE,
output = "kableExtra")
```

```{r echo=FALSE}
f2020 <- function (x) format(x, digits =4, scientific = FALSE)
modelsummary(modelspoi20,
             fmt=f2020,
             exponentiate = T,
             statistic = 'conf.int',
             title = "Exponenciales de la Regresión Poisson - Año 2020",
             stars =TRUE, 
             output = "kableExtra")
```


## 7.3. Interpretación de regresiones:


En el presente trabajo se ha determinado que para modelar la percepción de corrupción es necesario realizar una regression lineal y una regression Poisson, con el objetivo de identificar el mejor modelo para este trabajo.

Para el año 2019, la variable de Independencia de la Justicia no presenta significancia en ninguno de los modelos. Esta situación también se muestra para las variables de Desempleo e Inflación. Asimismo, vemos que la Poisson le devuelve cierta significancia a la variable de Índice de democracia, pues en la lineal no presenta significancia alguna. Por su parte, las variables de Ausencia de corrupción, Legitimidad estatal e IDH sí presentan un efecto significativo, las dos primeras al 0.001 y la variables de legitimidad al 0.01.

Para el año 2020, la variable de Independencia de la Justicia sigue sin presenter significancia en ambos modelos. Esta situación se replica para la variables de Índice de democracia. Por su parte, la variable de Desempleo presenta una ligera significancia,en la lineal y la pierde en la Poisson. Caso contrario ocurre con variable inflación, pues adquiere una minima significancia en la Poisson. Por otro lado, las variables de Ausencia de corrupción, IDH y Legitimidad estatal se muestran una significancia relevante de 0.001.

En ese sentido, las tablas muestran que los valores obtenidos por la regresiones se difieren ligeramente en los resultados. Por ello, ambas servirían para predecir; no obstante, solo la Poisson le da mayor significancia al predictor de interés.

Finalmente, es necesario resaltar que las tablas de coeficientes exponenciados brindan resultados mucho más exactos de la regresión Poisson. Sin embargo, los grados de significancia de las diferentes variables siguen la misma tendencia que los datos anteriores, por lo que no se profundiza más su análisis.


